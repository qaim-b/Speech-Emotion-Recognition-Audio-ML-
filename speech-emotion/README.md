# Speech Emotion Recognition (SER) ğŸ¤ğŸ˜Š

Deep learning system to detect emotions (happy, sad, angry, neutral, calm, fearful, disgust, surprised) from speech audio clips.

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
cd speech-emotion
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Download Dataset

**Option A: RAVDESS (Recommended)**
- Download from: https://zenodo.org/record/1188976
- Extract to: `data/RAVDESS/`

**Option B: TESS (Alternative)**
- Download from: https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess
- Extract to: `data/TESS/`

### 3. Train the Model

```bash
python -m src.train --data_dir data/RAVDESS --epochs 25 --batch_size 32
```

**Training arguments:**
- `--data_dir`: Path to dataset (default: `data/RAVDESS`)
- `--epochs`: Number of training epochs (default: 25)
- `--batch_size`: Batch size (default: 32)
- `--lr`: Learning rate (default: 0.001)
- `--feature_type`: Feature type - `mfcc` or `logmel` (default: `mfcc`)

**Output:**
- Model checkpoints saved to `models/`
- Training curves saved to `results/training_curves.png`
- Metrics saved to `results/training_metrics.json`

### 4. Evaluate the Model

```bash
python -m src.eval --data_dir data/RAVDESS --checkpoint models/best_model.pth
```

**Evaluation arguments:**
- `--checkpoint`: Path to model checkpoint (default: `models/best_model.pth`)
- `--split`: Dataset split to evaluate - `train`, `val`, or `test` (default: `test`)

**Output:**
- Confusion matrix saved to `results/test_confusion_matrix.png`
- Classification report saved to `results/test_classification_report.txt`
- Metrics saved to `results/test_metrics.json`

### 5. Run API Server

```bash
uvicorn app:app --host 0.0.0.0 --port 8080
```

Access the API at: http://localhost:8080

**API Endpoints:**
- `GET /` - API info and endpoints
- `GET /health` - Health check and model status
- `POST /predict` - Upload audio file for emotion prediction

**Test the API with curl:**
```bash
curl -X POST "http://localhost:8080/predict" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_audio.wav"
```

### 6. Docker Deployment

```bash
# Build image
docker build -t emotion-api .

# Run container
docker run -p 8080:8080 emotion-api
```

## ğŸ“Š Architecture

**Pipeline:**
```
Audio Input â†’ librosa â†’ MFCC/Mel Features â†’ PyTorch CNN â†’ Softmax â†’ Emotion Label
```

**Model:**
- 3 Convolutional blocks with BatchNorm + MaxPool + Dropout
- Adaptive pooling for variable-length inputs
- 3 Fully connected layers with dropout
- ~2-3M trainable parameters

**Features:**
- MFCC: 40 coefficients
- Mel Spectrogram: 64 mel bins
- Sample rate: 16kHz
- Audio duration: 3 seconds (padded/trimmed)

## ğŸ“ Project Structure

```
speech-emotion/
â”œâ”€â”€ data/               # Datasets (RAVDESS, TESS)
â”œâ”€â”€ features/           # Cached feature files
â”œâ”€â”€ models/             # Model checkpoints and artifacts
â”œâ”€â”€ results/            # Training curves, confusion matrices, metrics
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py      # PyTorch dataset
â”‚   â”œâ”€â”€ features.py     # Audio feature extraction
â”‚   â”œâ”€â”€ models.py       # CNN architecture
â”‚   â”œâ”€â”€ train.py        # Training script
â”‚   â”œâ”€â”€ eval.py         # Evaluation script
â”‚   â””â”€â”€ utils.py        # Helper functions
â”œâ”€â”€ app.py              # FastAPI inference API
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ Dockerfile          # Docker configuration
â””â”€â”€ README.md
```

## ğŸ¯ Expected Performance

On RAVDESS dataset:
- **Training accuracy**: 85-95%
- **Validation accuracy**: 60-75%
- **Test accuracy**: 55-70%

Performance depends on:
- Dataset size and quality
- Number of training epochs
- Hyperparameter tuning
- Feature type (MFCC vs Mel)

## ğŸ”§ Advanced Usage

**Train with custom parameters:**
```bash
python -m src.train \
  --data_dir data/RAVDESS \
  --epochs 50 \
  --batch_size 64 \
  --lr 0.0005 \
  --feature_type logmel
```

**Evaluate on validation set:**
```bash
python -m src.eval \
  --checkpoint models/best_model.pth \
  --split val
```

**Resume training from checkpoint:**
```python
# Modify src/train.py to load checkpoint in optimizer
```

## ğŸ“¦ Dependencies

- Python 3.10+
- PyTorch 2.4.0
- librosa 0.10.2
- FastAPI 0.111.0
- scikit-learn 1.5.0
- numpy, pandas, matplotlib

See `requirements.txt` for full list.

## ğŸ“ Use Cases

- **Call centers**: Analyze customer emotion in real-time
- **Mental health**: Monitor emotional state in therapy sessions
- **Virtual assistants**: Respond appropriately to user emotions
- **Gaming**: Adaptive gameplay based on player emotion
- **Market research**: Analyze emotional responses in focus groups

## ğŸš¨ Troubleshooting

**Issue**: `ModuleNotFoundError: No module named 'src'`
- **Fix**: Run commands from the `speech-emotion` directory

**Issue**: `FileNotFoundError: data/RAVDESS`
- **Fix**: Download and extract dataset to correct location

**Issue**: API returns "Model not loaded"
- **Fix**: Train model first: `python -m src.train`

**Issue**: CUDA out of memory
- **Fix**: Reduce batch size: `--batch_size 16`

**Issue**: Low accuracy
- **Fix**: Train for more epochs, use data augmentation, tune hyperparameters

## ğŸ“ License

Educational project - use responsibly with proper attribution.

## ğŸ¤ Contributing

This is a learning project. Fork it, experiment, and build on it!

## ğŸ‰ Next Steps

1. âœ… Train baseline model
2. â¬œ Add data augmentation (pitch shift, time stretch, noise)
3. â¬œ Try different architectures (LSTM, Transformer)
4. â¬œ Implement real-time streaming inference
5. â¬œ Deploy to cloud (AWS/GCP/Azure)
6. â¬œ Build web frontend
7. â¬œ Add multilingual support

---

**Built for AI Engineer Readiness Projects** ğŸš€
